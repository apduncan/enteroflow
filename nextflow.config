// SHARED PARAMETERS
// The matrix to be decomposed
params.matrix = "resources/test_data.tsv"
// Number of random shuffles of matrix
params.n = 100
// Max NMF iterations, shared across all tasks
params.nmf_max_iter = 3000
// Random seed
params.seed = 4298

// RANK SELECTION PARAMETERS
// Lower and upper bound of ranks to search during rank selection, inclusive
params.ranks = "2,7"

// REGULARISATION PARAMETERS
// Regularisation values to search
params.alpha = [0, 0.03125, 0.0625, 0.125, 0.25, 0.5, 1, 2, 5]
// Regularisation ratio
// This is the ratio of sparsifying (L1) to densifying (L2) regularisation to
// be applied. Defaults to L1. This is used in rank selection, but with an
// alpha value of 0.0, meaning no regularisation is applied during rank
// selection
params.l1_ratio = 1.0
// Ranks to search in regularisation
// Regularisation is not done by default, as it a lot more iterations and is
// only needed for ranks you are interested in. After runnning rank selection,
// you can add some ranks to be checked in here - they do not need to 
// be sequential 
params.regu_rank = [5]

// FULL DECOMPOSITION PARAMETERS
// Number of randomly initialised decompositions on full matrix
// when getting decompositions for a given rank
params.random_starts = 50

// OUTPUT PARAMS
params.publish_dir = "output"


profiles {
    conda {
        process.conda = 'env.yaml'
        conda.mamba = true
    }
    slurm {
        // conda stuff
        conda.useMicromamba = true
        // slurm stuff
        process {
            conda = '/path/to/env/'
            executor = 'slurm'
            queue = 'ei-short'
            memory = '1.5GB'
            cpus = 4
            withLabel: largemem {
                // Typically our processes requiring a lot of memory are
                // splitting and merging data, and not very computationally
                // demanding
                queue = 'ei-short'
                memory = '8GB'
                cpus = 2
            }
        }
    }
}

dag.verbose = true